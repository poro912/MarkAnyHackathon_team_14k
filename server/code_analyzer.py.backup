import os
import re
import json
import time
import hashlib
import pickle
import hashlib
from pathlib import Path
from strands import Agent

class CodeAnalyzer:
    # ÌÅ¥ÎûòÏä§ Î†àÎ≤® Ï∫êÏãú (ÏÑúÎ≤ÑÍ∞Ä ÏºúÏ†∏ ÏûàÎäî ÎèôÏïà Ïú†ÏßÄ)
    _cache = {}
    _cache_file = "code_analysis_cache.pkl"
    
    def __init__(self):
        self.supported_extensions = {'.py', '.js', '.java', '.cpp', '.c', '.cs', '.php', '.rb', '.go', '.ts'}
        self.language_map = {'.py': 'Python', '.js': 'JavaScript', '.cpp': 'C++', '.java': 'Java', '.c': 'C', '.cs': 'C#', '.php': 'PHP', '.rb': 'Ruby', '.go': 'Go', '.ts': 'TypeScript'}
        try:
            self.agent = Agent(model="claude-3-haiku")
            self.use_ai = True
        except:
            self.agent = None
            self.use_ai = False
        
        # Ï∫êÏãú ÌååÏùºÏóêÏÑú Î°úÎìú
        self._load_cache()

    def _load_cache(self):
        """Ï∫êÏãú ÌååÏùºÏóêÏÑú Ï∫êÏãú Î°úÎìú"""
        try:
            if os.path.exists(self._cache_file):
                with open(self._cache_file, 'rb') as f:
                    self._cache = pickle.load(f)
                print(f"üìã Ï∫êÏãú ÌååÏùºÏóêÏÑú {len(self._cache)}Í∞ú Ìï≠Î™© Î°úÎìú")
        except Exception as e:
            print(f"Ï∫êÏãú Î°úÎìú Ïò§Î•ò: {e}")
            self._cache = {}
    
    def _save_cache(self):
        """Ï∫êÏãúÎ•º ÌååÏùºÏóê Ï†ÄÏû•"""
        try:
            with open(self._cache_file, 'wb') as f:
                pickle.dump(self._cache, f)
        except Exception as e:
            print(f"Ï∫êÏãú Ï†ÄÏû• Ïò§Î•ò: {e}")
    
    def _get_cache_key(self, project_path):
        """ÌîÑÎ°úÏ†ùÌä∏ ÌååÏùº Íµ¨Ï°∞ÏôÄ ÎÇ¥Ïö©ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±"""
        file_info = []
        for root, dirs, files in os.walk(project_path):
            for file in files:
                if Path(file).suffix in self.supported_extensions:
                    file_path = os.path.join(root, file)
                    # ÏÉÅÎåÄ Í≤ΩÎ°ú ÏÇ¨Ïö© (ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú Ï†úÍ±∞)
                    rel_path = os.path.relpath(file_path, project_path)
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
                        file_info.append(f"{rel_path}:{content_hash}")
                    except:
                        continue
        file_info.sort()
        # ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú ÎåÄÏã† ÌååÏùº Íµ¨Ï°∞Îßå ÏÇ¨Ïö©
        cache_data = '|'.join(file_info)
        return hashlib.md5(cache_data.encode()).hexdigest()
    
    def _get_cached_result(self, cache_key):
        """Ï∫êÏãúÏóêÏÑú Í≤∞Í≥º Ï°∞Ìöå"""
        return self._cache.get(cache_key)
    
    def _set_cached_result(self, cache_key, result):
        """Ï∫êÏãúÏóê Í≤∞Í≥º Ï†ÄÏû•"""
        self._cache[cache_key] = result
        self._save_cache()  # Ï¶âÏãú ÌååÏùºÏóê Ï†ÄÏû•
    

    def _analyze_summary(self, results):
        """ÌååÏùº Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ï¢ÖÌï©ÌïòÏó¨ ÏµúÏ¢Ö Î∂ÑÏÑù ÏàòÌñâ"""
        if not self.use_ai or not results:
            return {"result": "Î∂ÑÏÑù Î∂àÍ∞Ä", "desc": "AI Î∂ÑÏÑùÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§."}
        
        # Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
        avg_difficulty = sum(r['difficulty_score'] for r in results) / len(results)
        total_hours = sum(r['estimated_dev_hours'] for r in results)
        avg_complexity = sum(r['cyclomatic_complexity'] for r in results) / len(results)
        developer_levels = [str(r['developer_level']) for r in results]
        tech_stacks = [str(r.get('tech_stack_identification', '')) for r in results if r.get('tech_stack_identification')]
        
        prompt = f"""Îã§Ïùå ÌîÑÎ°úÏ†ùÌä∏ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ï¢ÖÌï©ÌïòÏó¨ ÏµúÏ¢Ö ÌèâÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî:

- ÌèâÍ∑† ÎÇúÏù¥ÎèÑ: {avg_difficulty:.1f}/10
- Ï¥ù ÏòàÏÉÅ Í∞úÎ∞úÏãúÍ∞Ñ: {total_hours:.1f}ÏãúÍ∞Ñ
- ÌèâÍ∑† Î≥µÏû°ÎèÑ: {avg_complexity:.1f}
- Í∞úÎ∞úÏûê ÏàòÏ§Ä: {', '.join(developer_levels)}
- Í∏∞Ïà† Ïä§ÌÉù: {', '.join(tech_stacks)}

Îã§Ïùå Ï§ë ÌïòÎÇòÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
- Ïã†ÏûÖÏÇ¨ÏõêÎèÑ Ï∂©Î∂ÑÌûà Í∞úÎ∞ú Í∞ÄÎä•Ìï®
- 1~2ÎÖÑÏ∞® Í∞úÎ∞úÏûêÏóê Ï†ÅÌï©Ìï®
- 3~5ÎÖÑÏ∞® Í∞úÎ∞úÏûêÏóê Ï†ÅÌï©Ìï®
- 5~10ÎÖÑÏ∞® Í∞úÎ∞úÏûêÏóê Ï†ÅÌï©Ìï®
- 10ÎÖÑÏ∞® Ïù¥ÏÉÅ Í∞úÎ∞úÏûêÏóê Ï†ÅÌï©Ìï®

JSON ÌòïÌÉúÎ°ú ÏùëÎãµ:
{{"result": "ÏÑ†ÌÉùÎêú Í≤∞Í≥º", "desc": "Î∂ÑÏÑù Í∑ºÍ±∞ ÏÑ§Î™Ö"}}"""

        try:
            response = self.bedrock_client.invoke_model(
                modelId="anthropic.claude-3-haiku-20240307-v1:0",
                body=json.dumps({
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 1000,
                    "messages": [{"role": "user", "content": prompt}]
                })
            )
            
            result = json.loads(response['body'].read())
            ai_response = result['content'][0]['text']
            
            # JSON Ï∂îÏ∂ú Î∞è Ï†úÏñ¥ Î¨∏Ïûê Ï†úÍ±∞
            json_start = ai_response.find('{')
            json_end = ai_response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = ai_response[json_start:json_end]
                # Ï†úÏñ¥ Î¨∏Ïûê Ï†úÍ±∞
                json_str = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_str)
                return json.loads(json_str)
            else:
                return {"result": "Î∂ÑÏÑù Ïã§Ìå®", "desc": "ÏùëÎãµ ÌååÏã± Ïò§Î•ò"}
                
        except Exception as e:
            return {"result": "Î∂ÑÏÑù Ïò§Î•ò", "desc": f"Ïò§Î•ò: {str(e)}"}

    def _analyze_with_ai(self, content: str, file_path: str):
        """AIÎ•º ÏÇ¨Ïö©Ìïú ÏΩîÎìú Î∂ÑÏÑù"""
        if not self.use_ai:
            return self._fallback_analysis(content)
        
        prompt = f"""Îã§Ïùå ÏΩîÎìúÎ•º Î∂ÑÏÑùÌïòÏó¨ JSON ÌòïÌÉúÎ°ú Í≤∞Í≥ºÎ•º Î∞òÌôòÌï¥Ï£ºÏÑ∏Ïöî:

ÌååÏùº: {file_path}
ÏΩîÎìú:
```
{content[:2000]}  # Ï≤òÏùå 2000ÏûêÎßå Î∂ÑÏÑù
```

Îã§Ïùå Ìï≠Î™©Îì§ÏùÑ 1-10 Ï†êÏàòÎ°ú ÌèâÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî:
- cyclomatic_complexity: ÏàúÌôòÎ≥µÏû°ÎèÑ (1-50)
- maintainability_index: Ïú†ÏßÄÎ≥¥ÏàòÏÑ± ÏßÄÏàò (1-100)
- estimated_dev_hours: ÏòàÏÉÅ Í∞úÎ∞ú ÏãúÍ∞Ñ (ÏãúÍ∞Ñ Îã®ÏúÑ)
- difficulty_score: ÎÇúÏù¥ÎèÑ Ï†êÏàò (1-10)
- developer_level: ÌïÑÏöî Í∞úÎ∞úÏûê ÏàòÏ§Ä (Entry/Junior/Mid/Senior/Architect)
- pattern_score: Ìå®ÌÑ¥ ÏÇ¨Ïö© Ï†êÏàò (1-10)
- optimization_score: ÏµúÏ†ÅÌôî Ï†êÏàò (1-10)
- best_practices_score: Î™®Î≤îÏÇ¨Î°Ä Ï§ÄÏàòÎèÑ (1-10)
- tech_stack_identification: ÏÇ¨Ïö©Îêú Í∏∞Ïà† Ïä§ÌÉù (ÌîÑÎ†àÏûÑÏõåÌÅ¨, ÎùºÏù¥Î∏åÎü¨Î¶¨ Îì±ÏùÑ Í∞ÑÎã®Ìûà ÎÇòÏó¥)

JSON ÌòïÌÉúÎ°úÎßå ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:"""

        try:
            response = self.bedrock_client.invoke_model(
                #modelId='anthropic.claude-3-5-sonnet-20240620-v1:0',
                modelId='anthropic.claude-3-haiku-20240307-v1:0',
                body=json.dumps({
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 1000,
                    "messages": [{"role": "user", "content": prompt}]
                })
            )
            
            result = json.loads(response['body'].read())
            ai_response = result['content'][0]['text']
            
            # JSON Ï∂îÏ∂ú
            json_start = ai_response.find('{')
            json_end = ai_response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                ai_analysis = json.loads(ai_response[json_start:json_end])
                print(f"AI Î∂ÑÏÑù ÏÑ±Í≥µ: {file_path}")
                return ai_analysis
            
        except Exception as e:
            print(f"AI Î∂ÑÏÑù Ïã§Ìå®: {e}")
        
        return self._fallback_analysis(content)
    
    def _fallback_analysis(self, content):
        """AI Ïã§Ìå®Ïãú Í∏∞Î≥∏ Î∂ÑÏÑù"""
        complexity = 1 + sum(len(re.findall(rf'\b{k}\b', content, re.IGNORECASE)) 
                           for k in ['if', 'for', 'while', 'try', 'case'])
        
        difficulty = min(10, 1 + len([k for k in ['async', 'threading', 'regex'] if k in content.lower()]) + 
                        (2 if len(content.split('\n')) > 200 else 1 if len(content.split('\n')) > 100 else 0))
        
        levels = ["Entry", "Junior", "Mid", "Senior", "Architect"]
        dev_level = levels[min(4, (difficulty - 1) // 2)]
        
        return {
            'cyclomatic_complexity': min(complexity, 50),
            'maintainability_index': max(0, min(100, int(100 - max(0, (len(content.split('\n')) - 100) / 10)))),
            'estimated_dev_hours': round(len(content.split('\n')) * 0.1, 1),
            'difficulty_score': difficulty,
            'developer_level': dev_level,
            'pattern_score': min(10, 1 + len([p for p in ['class', 'interface', 'factory'] if p in content.lower()])),
            'optimization_score': min(10, 5 + len([o for o in ['cache', 'async', 'parallel'] if o in content.lower()])),
            'best_practices_score': min(10, 1 + len([b for b in ['try:', 'def ', 'class '] if b in content]))
        }
    
    def analyze_file(self, file_path: str):
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        lines = content.split('\n')
        total_lines = len(lines)
        code_lines = len([l for l in lines if l.strip() and not l.strip().startswith(('#', '//', '/*', '*'))])
        comment_lines = len([l for l in lines if l.strip().startswith(('#', '//', '/*', '*'))])
        
        # AI Î∂ÑÏÑù ÏàòÌñâ
        ai_analysis = self._analyze_with_ai(content, file_path)
        
        # Í∏∞Ïà† Ïä§ÌÉù ÏãùÎ≥Ñ
        ext = Path(file_path).suffix
        tech_stack = []
        if ext in {'.py': 'Python', '.js': 'JavaScript', '.cpp': 'C++', '.java': 'Java'}.keys():
            tech_stack.append({'.py': 'Python', '.js': 'JavaScript', '.cpp': 'C++', '.java': 'Java'}[ext])
        
        return {
            'file_path': file_path,
            'total_lines': total_lines,
            'code_lines': code_lines,
            'comment_lines': comment_lines,
            'code_comment_ratio': round(code_lines / max(comment_lines, 1), 2),
            'cyclomatic_complexity': ai_analysis['cyclomatic_complexity'],
            'maintainability_index': ai_analysis['maintainability_index'],
            'estimated_dev_hours': ai_analysis['estimated_dev_hours'],
            'difficulty_score': ai_analysis['difficulty_score'],
            'developer_level': ai_analysis['developer_level'],
            'pattern_score': ai_analysis['pattern_score'],
            'optimization_score': ai_analysis['optimization_score'],
            'best_practices_score': ai_analysis['best_practices_score'],
            'tech_stack_identification': ai_analysis.get('tech_stack_identification', 'AI Î∂ÑÏÑù Î∂àÍ∞Ä'),            'language': self.language_map.get(ext, 'Unknown'),
            'tech_stack': tech_stack
        }
    
    def analyze_project(self, project_path: str):
        # Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±
        cache_key = self._get_cache_key(project_path)
        
        # Ï∫êÏãúÏóêÏÑú Í≤∞Í≥º Ï°∞Ìöå
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            print(f"üìã Ï∫êÏãúÎêú Î∂ÑÏÑù Í≤∞Í≥º ÏÇ¨Ïö©: {project_path}")
            return cached_result
        
        print(f"üîç ÏÉàÎ°úÏö¥ Î∂ÑÏÑù ÏãúÏûë: {project_path}")
        results = []
        for root, dirs, files in os.walk(project_path):
            for file in files:
                if Path(file).suffix in self.supported_extensions:
                    results.append(self.analyze_file(os.path.join(root, file)))
                # AI rate limit Í≥†Î†§, Ïã§Ï†ú Í∞íÏù¥ ÏñºÎßàÏù∏ÏßÄ ÌôïÏù∏ÌõÑ Ï≤òÎ¶¨ ÌïÑÏöî
                # Ïó¨Í∏∞ÏÑúÎäî Îß§ ÏöîÏ≤≠ÎßàÎã§ 2Ï¥à ÎåÄÍ∏∞
                if self.use_ai:
                    time.sleep(2)
        
        if results:
            summary = {
                'total_files': len(results),
                'total_lines': sum(r['total_lines'] for r in results),
                'avg_complexity': round(sum(r['cyclomatic_complexity'] for r in results) / len(results), 2),
                'total_estimated_hours': round(sum(r['estimated_dev_hours'] for r in results), 1),
                'max_difficulty': max(r['difficulty_score'] for r in results)
            }
            
            # ÏµúÏ¢Ö Î∂ÑÏÑù ÏàòÌñâ
            final_analysis = self._analyze_summary(results)
            summary.update(final_analysis)
        else:
            summary = {}
        
        result = {'files': results, 'summary': summary}
        
        # Í≤∞Í≥ºÎ•º Ï∫êÏãúÏóê Ï†ÄÏû•
        self._set_cached_result(cache_key, result)
        print(f"üíæ Î∂ÑÏÑù Í≤∞Í≥º Ï∫êÏãú Ï†ÄÏû• ÏôÑÎ£å: {project_path}")
        
        return result
