import os
import re
import json
import time
import hashlib
import pickle
import hashlib
from pathlib import Path
from strands import Agent

class CodeAnalyzer:
    # í´ë˜ìŠ¤ ë ˆë²¨ ìºì‹œ (ì„œë²„ê°€ ì¼œì ¸ ìˆëŠ” ë™ì•ˆ ìœ ì§€)
    _cache = {}
    _cache_file = "code_analysis_cache.pkl"
    
    def __init__(self):
        self.supported_extensions = {'.py', '.js', '.java', '.cpp', '.c', '.cs', '.php', '.rb', '.go', '.ts'}
        self.language_map = {'.py': 'Python', '.js': 'JavaScript', '.cpp': 'C++', '.java': 'Java', '.c': 'C', '.cs': 'C#', '.php': 'PHP', '.rb': 'Ruby', '.go': 'Go', '.ts': 'TypeScript'}
        try:
            self.agent = Agent(model="claude-3-haiku")
            self.use_ai = True
        except:
            self.agent = None
            self.use_ai = False
        
        # ìºì‹œ íŒŒì¼ì—ì„œ ë¡œë“œ
        self._load_cache()

    def _load_cache(self):
        """ìºì‹œ íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ"""
        try:
            if os.path.exists(self._cache_file):
                with open(self._cache_file, 'rb') as f:
                    self._cache = pickle.load(f)
                print(f"ğŸ“‹ ìºì‹œ íŒŒì¼ì—ì„œ {len(self._cache)}ê°œ í•­ëª© ë¡œë“œ")
        except Exception as e:
            print(f"ìºì‹œ ë¡œë“œ ì˜¤ë¥˜: {e}")
            self._cache = {}
    
    def _save_cache(self):
        """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            with open(self._cache_file, 'wb') as f:
                pickle.dump(self._cache, f)
        except Exception as e:
            print(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _get_cache_key(self, project_path):
        """í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        file_info = []
        for root, dirs, files in os.walk(project_path):
            for file in files:
                if Path(file).suffix in self.supported_extensions:
                    file_path = os.path.join(root, file)
                    # ìƒëŒ€ ê²½ë¡œ ì‚¬ìš© (ì„ì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì œê±°)
                    rel_path = os.path.relpath(file_path, project_path)
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
                        file_info.append(f"{rel_path}:{content_hash}")
                    except:
                        continue
        file_info.sort()
        # í”„ë¡œì íŠ¸ ê²½ë¡œ ëŒ€ì‹  íŒŒì¼ êµ¬ì¡°ë§Œ ì‚¬ìš©
        cache_data = '|'.join(file_info)
        return hashlib.md5(cache_data.encode()).hexdigest()
    
    def _get_cached_result(self, cache_key):
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        return self._cache.get(cache_key)
    
    def _set_cached_result(self, cache_key, result):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        self._cache[cache_key] = result
        self._save_cache()  # ì¦‰ì‹œ íŒŒì¼ì— ì €ì¥
    

    def _analyze_summary(self, results):
        """íŒŒì¼ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¶„ì„ ìˆ˜í–‰"""
        if not self.use_ai or not results:
            return {"result": "ë¶„ì„ ë¶ˆê°€", "desc": "AI ë¶„ì„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        # ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘
        avg_difficulty = sum(r['difficulty_score'] for r in results) / len(results)
        total_hours = sum(r['estimated_dev_hours'] for r in results)
        avg_complexity = sum(r['cyclomatic_complexity'] for r in results) / len(results)
        developer_levels = [str(r['developer_level']) for r in results]
        tech_stacks = [str(r.get('tech_stack_identification', '')) for r in results if r.get('tech_stack_identification')]
        
        prompt = f"""ë‹¤ìŒ í”„ë¡œì íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… í‰ê°€í•´ì£¼ì„¸ìš”:

- í‰ê·  ë‚œì´ë„: {avg_difficulty:.1f}/10
- ì´ ì˜ˆìƒ ê°œë°œì‹œê°„: {total_hours:.1f}ì‹œê°„
- í‰ê·  ë³µì¡ë„: {avg_complexity:.1f}
- ê°œë°œì ìˆ˜ì¤€: {', '.join(developer_levels)}
- ê¸°ìˆ  ìŠ¤íƒ: {', '.join(tech_stacks)}

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
- ì‹ ì…ì‚¬ì›ë„ ì¶©ë¶„íˆ ê°œë°œ ê°€ëŠ¥í•¨
- 1~2ë…„ì°¨ ê°œë°œìì— ì í•©í•¨
- 3~5ë…„ì°¨ ê°œë°œìì— ì í•©í•¨
- 5~10ë…„ì°¨ ê°œë°œìì— ì í•©í•¨
- 10ë…„ì°¨ ì´ìƒ ê°œë°œìì— ì í•©í•¨

JSON í˜•íƒœë¡œ ì‘ë‹µ:
{{"result": "ì„ íƒëœ ê²°ê³¼", "desc": "ë¶„ì„ ê·¼ê±° ì„¤ëª…"}}"""

        try:
            response = self.bedrock_client.invoke_model(
                modelId="anthropic.claude-3-haiku-20240307-v1:0",
                body=json.dumps({
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 1000,
                    "messages": [{"role": "user", "content": prompt}]
                })
            )
            
            result = json.loads(response['body'].read())
            ai_response = result['content'][0]['text']
            
            # JSON ì¶”ì¶œ ë° ì œì–´ ë¬¸ì ì œê±°
            json_start = ai_response.find('{')
            json_end = ai_response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = ai_response[json_start:json_end]
                # ì œì–´ ë¬¸ì ì œê±°
                json_str = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_str)
                return json.loads(json_str)
            else:
                return {"result": "ë¶„ì„ ì‹¤íŒ¨", "desc": "ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜"}
                
        except Exception as e:
            return {"result": "ë¶„ì„ ì˜¤ë¥˜", "desc": f"ì˜¤ë¥˜: {str(e)}"}

    def _analyze_with_ai(self, content: str, file_path: str):
        """AIë¥¼ ì‚¬ìš©í•œ ì½”ë“œ ë¶„ì„"""
        if not self.use_ai:
            return self._fallback_analysis(content)
        
        prompt = f"""ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”:

íŒŒì¼: {file_path}
ì½”ë“œ:
```
{content[:2000]}  # ì²˜ìŒ 2000ìë§Œ ë¶„ì„
```

ë‹¤ìŒ í•­ëª©ë“¤ì„ 1-10 ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
- cyclomatic_complexity: ìˆœí™˜ë³µì¡ë„ (1-50)
- maintainability_index: ìœ ì§€ë³´ìˆ˜ì„± ì§€ìˆ˜ (1-100)
- estimated_dev_hours: ì˜ˆìƒ ê°œë°œ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)
- difficulty_score: ë‚œì´ë„ ì ìˆ˜ (1-10)
- developer_level: í•„ìš” ê°œë°œì ìˆ˜ì¤€ (Entry/Junior/Mid/Senior/Architect)
- pattern_score: íŒ¨í„´ ì‚¬ìš© ì ìˆ˜ (1-10)
- optimization_score: ìµœì í™” ì ìˆ˜ (1-10)
- best_practices_score: ëª¨ë²”ì‚¬ë¡€ ì¤€ìˆ˜ë„ (1-10)
- tech_stack_identification: ì‚¬ìš©ëœ ê¸°ìˆ  ìŠ¤íƒ (í”„ë ˆì„ì›Œí¬, ë¼ì´ë¸ŒëŸ¬ë¦¬ ë“±ì„ ê°„ë‹¨íˆ ë‚˜ì—´)

JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:"""

        try:
            response = self.bedrock_client.invoke_model(
                #modelId='anthropic.claude-3-5-sonnet-20240620-v1:0',
                modelId='anthropic.claude-3-haiku-20240307-v1:0',
                body=json.dumps({
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 1000,
                    "messages": [{"role": "user", "content": prompt}]
                })
            )
            
            result = json.loads(response['body'].read())
            ai_response = result['content'][0]['text']
            
            # JSON ì¶”ì¶œ
            json_start = ai_response.find('{')
            json_end = ai_response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                ai_analysis = json.loads(ai_response[json_start:json_end])
                print(f"AI ë¶„ì„ ì„±ê³µ: {file_path}")
                return ai_analysis
            
        except Exception as e:
            print(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return self._fallback_analysis(content)
    
    def _fallback_analysis(self, content):
        """AI ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë¶„ì„"""
        complexity = 1 + sum(len(re.findall(rf'\b{k}\b', content, re.IGNORECASE)) 
                           for k in ['if', 'for', 'while', 'try', 'case'])
        
        difficulty = min(10, 1 + len([k for k in ['async', 'threading', 'regex'] if k in content.lower()]) + 
                        (2 if len(content.split('\n')) > 200 else 1 if len(content.split('\n')) > 100 else 0))
        
        levels = ["Entry", "Junior", "Mid", "Senior", "Architect"]
        dev_level = levels[min(4, (difficulty - 1) // 2)]
        
        return {
            'cyclomatic_complexity': min(complexity, 50),
            'maintainability_index': max(0, min(100, int(100 - max(0, (len(content.split('\n')) - 100) / 10)))),
            'estimated_dev_hours': round(len(content.split('\n')) * 0.1, 1),
            'difficulty_score': difficulty,
            'developer_level': dev_level,
            'pattern_score': min(10, 1 + len([p for p in ['class', 'interface', 'factory'] if p in content.lower()])),
            'optimization_score': min(10, 5 + len([o for o in ['cache', 'async', 'parallel'] if o in content.lower()])),
            'best_practices_score': min(10, 1 + len([b for b in ['try:', 'def ', 'class '] if b in content]))
        }
    
    def analyze_file(self, file_path: str):
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        lines = content.split('\n')
        total_lines = len(lines)
        code_lines = len([l for l in lines if l.strip() and not l.strip().startswith(('#', '//', '/*', '*'))])
        comment_lines = len([l for l in lines if l.strip().startswith(('#', '//', '/*', '*'))])
        
        # AI ë¶„ì„ ìˆ˜í–‰
        ai_analysis = self._analyze_with_ai(content, file_path)
        
        # ê¸°ìˆ  ìŠ¤íƒ ì‹ë³„
        ext = Path(file_path).suffix
        tech_stack = []
        if ext in {'.py': 'Python', '.js': 'JavaScript', '.cpp': 'C++', '.java': 'Java'}.keys():
            tech_stack.append({'.py': 'Python', '.js': 'JavaScript', '.cpp': 'C++', '.java': 'Java'}[ext])
        
        return {
            'file_path': file_path,
            'total_lines': total_lines,
            'code_lines': code_lines,
            'comment_lines': comment_lines,
            'code_comment_ratio': round(code_lines / max(comment_lines, 1), 2),
            'cyclomatic_complexity': ai_analysis['cyclomatic_complexity'],
            'maintainability_index': ai_analysis['maintainability_index'],
            'estimated_dev_hours': ai_analysis['estimated_dev_hours'],
            'difficulty_score': ai_analysis['difficulty_score'],
            'developer_level': ai_analysis['developer_level'],
            'pattern_score': ai_analysis['pattern_score'],
            'optimization_score': ai_analysis['optimization_score'],
            'best_practices_score': ai_analysis['best_practices_score'],
            'tech_stack_identification': ai_analysis.get('tech_stack_identification', 'AI ë¶„ì„ ë¶ˆê°€'),            'language': self.language_map.get(ext, 'Unknown'),
            'tech_stack': tech_stack
        }
    
    def analyze_project(self, project_path: str):
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._get_cache_key(project_path)
        
        # ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            print(f"ğŸ“‹ ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©: {project_path}")
            return cached_result
        
        print(f"ğŸ” ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘: {project_path}")
        results = []
        for root, dirs, files in os.walk(project_path):
            for file in files:
                if Path(file).suffix in self.supported_extensions:
                    results.append(self.analyze_file(os.path.join(root, file)))
                # AI rate limit ê³ ë ¤, ì‹¤ì œ ê°’ì´ ì–¼ë§ˆì¸ì§€ í™•ì¸í›„ ì²˜ë¦¬ í•„ìš”
                # ì—¬ê¸°ì„œëŠ” ë§¤ ìš”ì²­ë§ˆë‹¤ 2ì´ˆ ëŒ€ê¸°
                if self.use_ai:
                    time.sleep(2)
        
        if results:
            summary = {
                'total_files': len(results),
                'total_lines': sum(r['total_lines'] for r in results),
                'avg_complexity': round(sum(r['cyclomatic_complexity'] for r in results) / len(results), 2),
                'total_estimated_hours': round(sum(r['estimated_dev_hours'] for r in results), 1),
                'max_difficulty': max(r['difficulty_score'] for r in results)
            }
            
            # ìµœì¢… ë¶„ì„ ìˆ˜í–‰
            final_analysis = self._analyze_summary(results)
            summary.update(final_analysis)
        else:
            summary = {}
        
        result = {'files': results, 'summary': summary}
        
        # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        self._set_cached_result(cache_key, result)
        print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥ ì™„ë£Œ: {project_path}")
        
        return result
